import time
import json
from app.URLê³¼ìš”ì•½ë¬¸ë§Œë“¤ê¸° import get_latest_video_data, summarize_content
from app.ì§€ìˆ˜ì •ë³´ê°€ì ¸ì˜¤ê¸° import fetch_index_info
from pytz import timezone
from datetime import datetime
from dateutil import parser
from app.redis_client import redis_client
from app.test_config import channels

# url, ìš”ì•½ ì €ì¥ ì½”ë“œ
def fetch_and_store_youtube_data():
    try:
        today_date = datetime.now(timezone("Asia/Seoul")).strftime("%Y-%m-%d")
        today_key = f"processed_urls:{today_date}"
        updated = False

        for channel in channels:
            # â›”ï¸ ì˜¤ëŠ˜ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìœ¼ë©´ stop ìœ íŠœë¸Œ API íšŒí”¼
            country = channel["country"]
            existing_url = redis_client.hget(today_key, country)
            if existing_url:
                print(f"â­ï¸ {country} â€” {today_key} : {existing_url.decode()}")
                continue
            video_data = get_latest_video_data(channel)

            # â›”ï¸ ì´ë¯¸ ì €ì¥ëœ URLê³¼ ë™ì¼í•˜ê±°ë‚˜ ì˜¤ëŠ˜ì ë‰´ìŠ¤ê°€ ì•„ë‹ˆë©´ stop OpenAI API íšŒí”¼
            video_published_date = datetime.strptime(video_data['publishedAt'], "%Y-%m-%d %H:%M:%S")
            video_date_str = video_published_date.strftime("%Y-%m-%d")  # ë¹„êµë¥¼ ìœ„í•œ "YYYY-MM-DD" í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            existing_url_str = redis_client.hget(today_key, country).decode() if existing_url else None
            if existing_url_str==video_data['url']:
                print(f"â­ï¸ {country} â€” ì´ì „ URLê³¼ ë™ì¼: {existing_url.decode()}")
                continue

            # â›”ï¸ ì˜¤ëŠ˜ ì˜¬ë¼ì˜¨ ì˜ìƒì´ ì•„ë‹˜
            if video_date_str != today_date:
                print(f"â­ï¸ ì—…ë¡œë“œ ë‚ ì§œ:{video_date_str} â€” íƒìƒ‰ë‚ ì§œ:{today_date}")
                continue

            # â›”ï¸ ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìœ¼ë©´ stop OpenAI API íšŒí”¼ í›„ ìš”ì•½ë‚´ìš©ì—†ì´ ì €ì¥
            if video_data['summary_content']:

                summary_result = summarize_content(video_data['summary_content'])
                video_data['summary_result'] = summary_result
            else:
                video_data['summary_result'] = "ìš”ì•½í•  ë‚´ìš©(ìë§‰ ë˜ëŠ” description) ì—†ìŒ."

            dt = parser.parse(video_data["publishedAt"])
            video_data["publishedAtFormatted"] = dt.astimezone(timezone("Asia/Seoul")).strftime("%Y-%m-%d %H:%M")
            video_data["processedAt"] = datetime.now(timezone("Asia/Seoul")).strftime("%Y-%m-%d %H:%M")

            # âœ… Redisì— ë‚˜ë¼ë³„ë¡œ ê°œë³„ ì €ì¥
            redis_client.set(f"youtube_data:{country}", json.dumps(video_data))
            redis_client.set(f"youtube_data_timestamp:{country}", str(int(time.time())))

            redis_client.hset(today_key, country, video_data["url"])
            redis_client.expire(today_key, 86400)  # 86400ì´ˆ = 1ì¼

            print(f"ğŸ”” {country} ìƒˆ URL ì €ì¥ë¨: {video_data['url']}")
            updated = True


        return "âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ" if updated else "âœ… ëª¨ë“  ë°ì´í„°ëŠ” ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤."
    except Exception as e:
        return f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def calculate_moving_average(data, period=100):
    result = []
    for i in range(len(data)):
        if i < period:
            pass
        else:
            avg = sum(d["close"] for d in data[i - period + 1:i + 1]) / period
            result.append(avg)
    return result

def calculate_envelope(moving_avg, percentage):
    upper = []
    lower = []
    for avg in moving_avg:
        upper.append(avg * (1 + percentage))
        lower.append(avg * (1 - percentage))
    return upper, lower


def fetch_and_store_index_data():
    try:
        new_data = fetch_index_info(day_num = 200)  # List of dicts, ë‚ ì§œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ì´ë¼ê³  ê°€ì •
        index_name = "nasdaq100"
        redis_key = f"index_data:{index_name.lower()}"

        moving_avg = calculate_moving_average(new_data, period=100)
        upper10, lower10 = calculate_envelope(moving_avg, 0.10)
        upper3, lower3 = calculate_envelope(moving_avg, 0.03)

        trimmed_data = new_data[-100:]
        # ê° ë°ì´í„°ì— í•´ë‹¹ ê³„ì‚°ê°’ ì¶”ê°€
        for i in range(len(trimmed_data)):
            trimmed_data[i]["ma100"] = moving_avg[i]
            trimmed_data[i]["envelope10_upper"] = upper10[i]
            trimmed_data[i]["envelope10_lower"] = lower10[i]
            trimmed_data[i]["envelope3_upper"] = upper3[i]
            trimmed_data[i]["envelope3_lower"] = lower3[i]

        # ìµœëŒ€ 100ê°œ ìœ ì§€
        redis_client.set(redis_key, json.dumps(trimmed_data))
        redis_client.set(f"{redis_key}:updatedAt", datetime.now(timezone("Asia/Seoul")).strftime("%Y-%m-%d %H:%M"))
        print(f"âœ… {len(trimmed_data)}ê°œ ì§€ìˆ˜ ë°ì´í„° ì €ì¥ ì™„ë£Œ")

        return "âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ"
    except Exception as e:
        return f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def scheduled_store():
    now = datetime.now(timezone('Asia/Seoul'))
    if 11 <= now.hour < 15:  # 11ì‹œ ~ 14ì‹œ 59ë¶„
        if now.hour == 11 and now.minute == 0:
            print("ğŸ“ˆ index data...")
            fetch_and_store_index_data()

        print("â° Scheduled store running at", now.strftime("%Y-%m-%d %H:%M"))
        fetch_and_store_youtube_data()




if __name__ == "__main__":
    result = fetch_and_store_youtube_data()
    print(result)

    # ì €ì¥ëœ ë°ì´í„° í™•ì¸
    for channel in channels:
        country = channel["country"]
        data = redis_client.get(f"youtube_data:{country}")
        print("ğŸ“¦ ì €ì¥ëœ ìœ íŠœë¸Œ ë°ì´í„°:")
        print(json.loads(data))

    fetch_and_store_index_data()